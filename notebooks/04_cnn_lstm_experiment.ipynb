{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0082cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Add src directory to path\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Import the aggregation function from utils script\n",
    "from src.utils import aggregate_interview_sequences\n",
    "\n",
    "from src.data_loader import load_androids_corpus\n",
    "from src.foundation_model_extractor import extract_wav2vec2_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06833cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading corpus metadata...\n",
      "Successfully loaded 112 Read task and 116 Interview task fold assignments.\n",
      "Interview clip sequences already exist. Loading from file.\n",
      "\n",
      "Loaded 111 Reading sequences and 857 Interview clip sequences.\n",
      "\n",
      "Aggregating interview clips into single sequences per participant...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "588140854ae54d27b56bf935b4e4bca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aggregating Sequences: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated clips for 114 participants.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f9ea72d0cb44f5ebae1c56824dc6e97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating Combined Sequences:   0%|          | 0/111 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- All sequence datasets are now prepared and ready for training ---\n"
     ]
    }
   ],
   "source": [
    "# Extract and Prepare All Sequence Data for DL Models\n",
    "# Load metadata and extract clip-level sequences\n",
    "BASE_DATA_PATH = 'E:/Dissertation_Data/Androids-Corpus' # Verify this path\n",
    "print(\"Loading corpus metadata...\")\n",
    "reading_df, interview_df = load_androids_corpus(BASE_DATA_PATH, verbose=False)\n",
    "participant_metadata = reading_df[['unique_participant_id', 'label', 'fold']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "SEQUENCES_READING_PATH = '../data/Processed_Features/features_wav2vec2_sequences_reading_task.pkl'\n",
    "SEQUENCES_INTERVIEW_CLIPS_PATH = '../data/Processed_Features/features_wav2vec2_sequences_interview_clips.pkl'\n",
    "\n",
    "# Run extraction for interview clips if the file doesn't already exist\n",
    "if not os.path.exists(SEQUENCES_INTERVIEW_CLIPS_PATH):\n",
    "    print(\"Extracting sequential embeddings for all interview clips...\")\n",
    "    interview_clip_sequences = extract_wav2vec2_sequences(interview_df)\n",
    "    if interview_clip_sequences:\n",
    "        print(f\"Saving interview clip sequences to: {SEQUENCES_INTERVIEW_CLIPS_PATH}\")\n",
    "        with open(SEQUENCES_INTERVIEW_CLIPS_PATH, 'wb') as f: pickle.dump(interview_clip_sequences, f)\n",
    "else:\n",
    "    print(f\"Interview clip sequences already exist. Loading from file.\")\n",
    "\n",
    "# Load all necessary sequence data\n",
    "with open(SEQUENCES_READING_PATH, 'rb') as f: reading_sequences = pickle.load(f)\n",
    "with open(SEQUENCES_INTERVIEW_CLIPS_PATH, 'rb') as f: interview_clip_sequences = pickle.load(f)\n",
    "print(f\"\\nLoaded {len(reading_sequences)} Reading sequences and {len(interview_clip_sequences)} Interview clip sequences.\")\n",
    "\n",
    "# Aggregate interview clips into session-level sequences using the util function\n",
    "interview_session_sequences = aggregate_interview_sequences(interview_clip_sequences, interview_df)\n",
    "print(f\"Aggregated clips for {len(interview_session_sequences)} participants.\")\n",
    "\n",
    "# Create the final Reading, Interview, and Combined datasets\n",
    "# Remap reading sequences to be keyed by participant ID for consistency\n",
    "reading_participant_map = reading_df.set_index('filename')['unique_participant_id']\n",
    "reading_session_sequences = {reading_participant_map[fname]: seq for fname, seq in reading_sequences.items() if fname in reading_participant_map.index}\n",
    "\n",
    "# Create combined sequences by concatenating reading and interview sequences\n",
    "combined_session_sequences = {}\n",
    "for participant_id in tqdm(participant_metadata['unique_participant_id'], desc=\"Creating Combined Sequences\"):\n",
    "    reading_seq = reading_session_sequences.get(participant_id)\n",
    "    interview_seq = interview_session_sequences.get(participant_id)\n",
    "    if reading_seq is not None and interview_seq is not None:\n",
    "        combined_session_sequences[participant_id] = np.vstack([reading_seq, interview_seq])\n",
    "\n",
    "# Store all prepared sequence sets in a final dictionary\n",
    "sequence_sets = {\n",
    "    'reading': reading_session_sequences,\n",
    "    'interview': interview_session_sequences,\n",
    "    'combined': combined_session_sequences\n",
    "}\n",
    "print(\"\\n--- All sequence datasets are now prepared and ready for training ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856fdfbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading task sequences already exist. Loading from file: ../data/Processed_Features/features_wav2vec2_sequences_reading_task.pkl\n",
      "\n",
      "Successfully loaded data for 111 reading files.\n",
      "Verified sequence shape for '01_CF56_1.wav': (4378, 768)\n"
     ]
    }
   ],
   "source": [
    "# Load or Extract Reading Task Sequences\n",
    "\n",
    "# Define the path to the reading task sequences file\n",
    "SEQUENCES_READING_PATH = '../data/Processed_Features/features_wav2vec2_sequences_reading_task.pkl'\n",
    "\n",
    "# Check if the file exists before running extraction\n",
    "if not os.path.exists(SEQUENCES_READING_PATH):\n",
    "    print(\"Reading task sequences not found. Running extraction...\")\n",
    "    \n",
    "    # Check if the reading_df DataFrame is loaded\n",
    "    if 'reading_df' in locals():\n",
    "        print(\"\\nExtracting sequential embeddings for the Reading Task...\")\n",
    "        \n",
    "        # Call the extractor function\n",
    "        reading_sequences = extract_wav2vec2_sequences(reading_df)\n",
    "        \n",
    "        # Save the new sequences to the pickle file\n",
    "        if reading_sequences:\n",
    "            print(f\"Saving new reading task sequences to: {SEQUENCES_READING_PATH}\")\n",
    "            with open(SEQUENCES_READING_PATH, 'wb') as f:\n",
    "                pickle.dump(reading_sequences, f)\n",
    "            print(\"Extraction and saving complete.\")\n",
    "    else:\n",
    "        print(\"ERROR: 'reading_df' not found. Cannot run extraction.\")\n",
    "else:\n",
    "    print(f\"Reading task sequences already exist. Loading from file: {SEQUENCES_READING_PATH}\")\n",
    "\n",
    "# Load the data regardless of whether it was just created or already existed\n",
    "try:\n",
    "    with open(SEQUENCES_READING_PATH, 'rb') as f:\n",
    "        reading_sequences = pickle.load(f)\n",
    "    \n",
    "    print(f\"\\nSuccessfully loaded data for {len(reading_sequences)} reading files.\")\n",
    "    \n",
    "    # Final Verification\n",
    "    first_filename = list(reading_sequences.keys())[0]\n",
    "    first_sequence = reading_sequences[first_filename]\n",
    "    print(f\"Verified sequence shape for '{first_filename}': {first_sequence.shape}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Could not load reading sequences from {SEQUENCES_READING_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bc6583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running experiment: WAV2VEC2_CNN_LSTM_READING ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1bc4ed55a314384af93ebc57bd925bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 5-Fold CV:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  > Early stopping triggered at epoch 34\n",
      "  > Early stopping triggered at epoch 19\n",
      "  > Early stopping triggered at epoch 33\n",
      "  > Early stopping triggered at epoch 28\n",
      "  > Early stopping triggered at epoch 38\n",
      "Results saved to ../data/Processed_Features/results_wav2vec2_cnn_lstm_reading.pkl\n",
      "\n",
      "--- Running experiment: WAV2VEC2_CNN_LSTM_INTERVIEW ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62e77d9c71cc4d91b16beae00383f30b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 5-Fold CV:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  > Early stopping triggered at epoch 44\n",
      "  > Early stopping triggered at epoch 37\n",
      "  > Early stopping triggered at epoch 48\n",
      "  > Early stopping triggered at epoch 31\n",
      "Results saved to ../data/Processed_Features/results_wav2vec2_cnn_lstm_interview.pkl\n",
      "\n",
      "--- Running experiment: WAV2VEC2_CNN_LSTM_COMBINED ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1423e6abfb534990b6f298f88294f5e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 5-Fold CV:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  > Early stopping triggered at epoch 35\n",
      "  > Early stopping triggered at epoch 44\n",
      "  > Early stopping triggered at epoch 35\n",
      "  > Early stopping triggered at epoch 49\n",
      "Results saved to ../data/Processed_Features/results_wav2vec2_cnn_lstm_combined.pkl\n",
      "\n",
      "--- All Deep Learning experiments are now complete! ---\n"
     ]
    }
   ],
   "source": [
    "# Run CNN-LSTM Experiments for All Data Types\n",
    "\n",
    "from src.dl_cv_strategies import run_pytorch_cv_with_early_stopping\n",
    "\n",
    "# Dictionary to store the results of the DL experiments\n",
    "dl_results = {}\n",
    "\n",
    "# Define experiment parameters\n",
    "N_EPOCHS = 50\n",
    "PATIENCE = 10\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 0.0001\n",
    "# The 'participant_metadata' DataFrame should be loaded from a previous cell.\n",
    "\n",
    "# Loop through the three prepared sequence sets\n",
    "for name, seq_dict in sequence_sets.items():\n",
    "    experiment_name = f'wav2vec2_cnn_lstm_{name}'\n",
    "    results_save_path = f'../data/Processed_Features/results_{experiment_name}.pkl'\n",
    "    \n",
    "    if not os.path.exists(results_save_path):\n",
    "        print(f\"\\n--- Running experiment: {experiment_name.upper()} ---\")\n",
    "        \n",
    "        # Call training function, handles the data alignment internally.\n",
    "        # Pass the full participant_metadata DataFrame for reliable label lookup.\n",
    "        results_df, predictions = run_pytorch_cv_with_early_stopping(\n",
    "            sequences_dict=seq_dict,\n",
    "            metadata_df=participant_metadata,\n",
    "            epochs=N_EPOCHS,\n",
    "            patience=PATIENCE,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            learning_rate=LEARNING_RATE\n",
    "        )\n",
    "        \n",
    "        # Save results\n",
    "        results_to_save = {'results_df': results_df, 'predictions': predictions}\n",
    "        with open(results_save_path, 'wb') as f:\n",
    "            pickle.dump(results_to_save, f)\n",
    "        print(f\"Results saved to {results_save_path}\")\n",
    "        dl_results[experiment_name] = results_to_save\n",
    "    else:\n",
    "        print(f\"\\nLoading pre-computed results for {experiment_name.upper()}\")\n",
    "        with open(results_save_path, 'rb') as f:\n",
    "            dl_results[experiment_name] = pickle.load(f)\n",
    "\n",
    "print(\"\\n--- All Deep Learning experiments are now complete! ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d6a3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: SVM results file not found. Final comparison will only show DL models.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_38366_row0_col0, #T_38366_row0_col2, #T_38366_row0_col4, #T_38366_row2_col1, #T_38366_row2_col3, #T_38366_row2_col5 {\n",
       "  background-color: #440154;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_38366_row0_col1, #T_38366_row0_col5, #T_38366_row1_col0, #T_38366_row1_col2, #T_38366_row1_col3, #T_38366_row1_col4 {\n",
       "  background-color: #fde725;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_38366_row0_col3 {\n",
       "  background-color: #52c569;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_38366_row1_col1 {\n",
       "  background-color: #81d34d;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_38366_row1_col5 {\n",
       "  background-color: #6ece58;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_38366_row2_col0 {\n",
       "  background-color: #238a8d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_38366_row2_col2 {\n",
       "  background-color: #addc30;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_38366_row2_col4 {\n",
       "  background-color: #24868e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_38366\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_38366_level0_col0\" class=\"col_heading level0 col0\" >Mean F1-Score</th>\n",
       "      <th id=\"T_38366_level0_col1\" class=\"col_heading level0 col1\" >Std Dev F1-Score</th>\n",
       "      <th id=\"T_38366_level0_col2\" class=\"col_heading level0 col2\" >Mean AUC</th>\n",
       "      <th id=\"T_38366_level0_col3\" class=\"col_heading level0 col3\" >Std Dev AUC</th>\n",
       "      <th id=\"T_38366_level0_col4\" class=\"col_heading level0 col4\" >Mean Accuracy</th>\n",
       "      <th id=\"T_38366_level0_col5\" class=\"col_heading level0 col5\" >Std Dev Accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Experiment</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_38366_level0_row0\" class=\"row_heading level0 row0\" >wav2vec2_cnn_lstm_reading</th>\n",
       "      <td id=\"T_38366_row0_col0\" class=\"data row0 col0\" >0.683</td>\n",
       "      <td id=\"T_38366_row0_col1\" class=\"data row0 col1\" >0.063</td>\n",
       "      <td id=\"T_38366_row0_col2\" class=\"data row0 col2\" >0.770</td>\n",
       "      <td id=\"T_38366_row0_col3\" class=\"data row0 col3\" >0.079</td>\n",
       "      <td id=\"T_38366_row0_col4\" class=\"data row0 col4\" >0.694</td>\n",
       "      <td id=\"T_38366_row0_col5\" class=\"data row0 col5\" >0.059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_38366_level0_row1\" class=\"row_heading level0 row1\" >wav2vec2_cnn_lstm_interview</th>\n",
       "      <td id=\"T_38366_row1_col0\" class=\"data row1 col0\" >0.739</td>\n",
       "      <td id=\"T_38366_row1_col1\" class=\"data row1 col1\" >0.075</td>\n",
       "      <td id=\"T_38366_row1_col2\" class=\"data row1 col2\" >0.815</td>\n",
       "      <td id=\"T_38366_row1_col3\" class=\"data row1 col3\" >0.076</td>\n",
       "      <td id=\"T_38366_row1_col4\" class=\"data row1 col4\" >0.744</td>\n",
       "      <td id=\"T_38366_row1_col5\" class=\"data row1 col5\" >0.074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_38366_level0_row2\" class=\"row_heading level0 row2\" >wav2vec2_cnn_lstm_combined</th>\n",
       "      <td id=\"T_38366_row2_col0\" class=\"data row2 col0\" >0.710</td>\n",
       "      <td id=\"T_38366_row2_col1\" class=\"data row2 col1\" >0.124</td>\n",
       "      <td id=\"T_38366_row2_col2\" class=\"data row2 col2\" >0.809</td>\n",
       "      <td id=\"T_38366_row2_col3\" class=\"data row2 col3\" >0.086</td>\n",
       "      <td id=\"T_38366_row2_col4\" class=\"data row2 col4\" >0.717</td>\n",
       "      <td id=\"T_38366_row2_col5\" class=\"data row2 col5\" >0.124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x22f92adf690>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Final Analysis - Compare SVM and DL Models\n",
    "\n",
    "# Load SVM results from notebook 03, ensure they are saved them to a pkl file\n",
    "SVM_RESULTS_PATH = '../data/Processed_Features/all_svm_results.pkl'\n",
    "if os.path.exists(SVM_RESULTS_PATH):\n",
    "    with open(SVM_RESULTS_PATH, 'rb') as f:\n",
    "        all_results = pickle.load(f)\n",
    "else:\n",
    "    print(\"Warning: SVM results file not found. Final comparison will only show DL models.\")\n",
    "    all_results = {}\n",
    "\n",
    "# Add the new DL results to the main dictionary\n",
    "all_results.update(dl_results)\n",
    "\n",
    "# re-run all the plotting and analysis cells from notebook 03\n",
    "# e.g. generate final summary table:\n",
    "\n",
    "final_summary_data = []\n",
    "for experiment_name, data in all_results.items():\n",
    "    results_df = data['results_df']\n",
    "    final_summary_data.append({\n",
    "        'Experiment': experiment_name,\n",
    "        'Mean F1-Score': results_df['f1_score'].mean(),\n",
    "        'Std Dev F1-Score': results_df['f1_score'].std(),\n",
    "        'Mean AUC': results_df['auc'].mean(),\n",
    "        'Std Dev AUC': results_df['auc'].std(),\n",
    "        'Mean Accuracy': results_df['accuracy'].mean(),\n",
    "        'Std Dev Accuracy': results_df['accuracy'].std()\n",
    "    })\n",
    "\n",
    "final_summary_df = pd.DataFrame(final_summary_data).set_index('Experiment')\n",
    "display(final_summary_df.style.background_gradient(cmap='viridis', subset=[c for c in final_summary_df.columns if 'Mean' in c], axis=0)\n",
    "                          .background_gradient(cmap='viridis_r', subset=[c for c in final_summary_df.columns if 'Std Dev' in c], axis=0)\n",
    "                          .format(\"{:.3f}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a25ea4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc_final_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
